{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark=SparkSession.builder \\\n",
    "                .master(\"local[4]\") \\\n",
    "                .appName(\"string operations\") \\\n",
    "                .config(\"spark.driver.memory\",\"2g\") \\\n",
    "                .config(\"spark.executor.memory\",\"4g\") \\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    processed_article = re.sub(r'@\\w+', '', line)\n",
    "    processed_article = re.sub(r'http\\S+', '', processed_article)\n",
    "    processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "    \n",
    "    return (processed_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okurken okunan sütunlar secilebilir.\n",
    "df=spark.read \\\n",
    "         .option(\"sep\",\",\") \\\n",
    "         .option(\"header\",\"True\") \\\n",
    "         .option(\"inferSchema\",\"True\") \\\n",
    "         .csv(\"realDonaldTrumps_tweets.csv\") \\\n",
    "         .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>t_date</th>\n",
       "      <th>t_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1121407497741447168</td>\n",
       "      <td>4/25/2019 13:35</td>\n",
       "      <td>b'RT @RepAndyBiggsAZ: I\\xe2\\x80\\x99m grateful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1121407411875647488</td>\n",
       "      <td>4/25/2019 13:35</td>\n",
       "      <td>b'RT @RepMattGaetz: Volume 1 of the Mueller Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1121406614093860864</td>\n",
       "      <td>4/25/2019 13:32</td>\n",
       "      <td>b'RT @RepMarkMeadows: We knew they wouldn\\xe2\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1121406415707484160</td>\n",
       "      <td>4/25/2019 13:31</td>\n",
       "      <td>\"b'RT @dcexaminer: \"\"The collusion delusion fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1121406299013513217</td>\n",
       "      <td>4/25/2019 13:31</td>\n",
       "      <td>b'RT @RepMarkMeadows: Reminder: Democrats dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1121406061519544322</td>\n",
       "      <td>4/25/2019 13:30</td>\n",
       "      <td>b'RT @Jim_Jordan: It\\xe2\\x80\\x99s time to figu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1121405753364025344</td>\n",
       "      <td>4/25/2019 13:29</td>\n",
       "      <td>\"b\"\"RT @GOPoversight: Democrats are obsessed w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1121405495405948929</td>\n",
       "      <td>4/25/2019 13:27</td>\n",
       "      <td>\"b'RT @Jim_Jordan: Peter Strzok told us that h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1121388967444799488</td>\n",
       "      <td>4/25/2019 12:22</td>\n",
       "      <td>b'Welcome to the race Sleepy Joe. I only hope ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1121385795481423873</td>\n",
       "      <td>4/25/2019 12:09</td>\n",
       "      <td>b'.....Despite the fact that the Mueller Repor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  t_id           t_date  \\\n",
       "0  1121407497741447168  4/25/2019 13:35   \n",
       "1  1121407411875647488  4/25/2019 13:35   \n",
       "2  1121406614093860864  4/25/2019 13:32   \n",
       "3  1121406415707484160  4/25/2019 13:31   \n",
       "4  1121406299013513217  4/25/2019 13:31   \n",
       "5  1121406061519544322  4/25/2019 13:30   \n",
       "6  1121405753364025344  4/25/2019 13:29   \n",
       "7  1121405495405948929  4/25/2019 13:27   \n",
       "8  1121388967444799488  4/25/2019 12:22   \n",
       "9  1121385795481423873  4/25/2019 12:09   \n",
       "\n",
       "                                              t_text  \n",
       "0  b'RT @RepAndyBiggsAZ: I\\xe2\\x80\\x99m grateful ...  \n",
       "1  b'RT @RepMattGaetz: Volume 1 of the Mueller Re...  \n",
       "2  b'RT @RepMarkMeadows: We knew they wouldn\\xe2\\...  \n",
       "3  \"b'RT @dcexaminer: \"\"The collusion delusion fe...  \n",
       "4  b'RT @RepMarkMeadows: Reminder: Democrats dema...  \n",
       "5  b'RT @Jim_Jordan: It\\xe2\\x80\\x99s time to figu...  \n",
       "6  \"b\"\"RT @GOPoversight: Democrats are obsessed w...  \n",
       "7  \"b'RT @Jim_Jordan: Peter Strzok told us that h...  \n",
       "8  b'Welcome to the race Sleepy Joe. I only hope ...  \n",
       "9  b'.....Despite the fact that the Mueller Repor...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.withColumn(\"t_date\",F.to_timestamp(F.col(\"t_date\"),\"MM/dd/yyyy HH:mm\")) \\\n",
    "        .withColumn(\"t_text\", F.lower(F.col(\"t_text\"))) \\\n",
    "        .withColumn(\"t_text\",F.regexp_replace(F.col(\"t_text\"),\"b'\",\"\")) \\\n",
    "        .withColumn(\"t_text\",F.regexp_replace(F.col(\"t_text\"),\"''\",\"\")) \\\n",
    "        .withColumn(\"t_text\",F.regexp_replace(F.col(\"t_text\"),\"''\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t_id: long (nullable = true)\n",
      " |-- t_date: timestamp (nullable = true)\n",
      " |-- t_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>t_date</th>\n",
       "      <th>t_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1121407497741447168</td>\n",
       "      <td>2019-04-25 13:35:00</td>\n",
       "      <td>rt @repandybiggsaz: i\\xe2\\x80\\x99m grateful fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1121407411875647488</td>\n",
       "      <td>2019-04-25 13:35:00</td>\n",
       "      <td>rt @repmattgaetz: volume 1 of the mueller repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1121406614093860864</td>\n",
       "      <td>2019-04-25 13:32:00</td>\n",
       "      <td>rt @repmarkmeadows: we knew they wouldn\\xe2\\x8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1121406415707484160</td>\n",
       "      <td>2019-04-25 13:31:00</td>\n",
       "      <td>\"rt @dcexaminer: \"\"the collusion delusion fell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1121406299013513217</td>\n",
       "      <td>2019-04-25 13:31:00</td>\n",
       "      <td>rt @repmarkmeadows: reminder: democrats demand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1121406061519544322</td>\n",
       "      <td>2019-04-25 13:30:00</td>\n",
       "      <td>rt @jim_jordan: it\\xe2\\x80\\x99s time to figure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1121405753364025344</td>\n",
       "      <td>2019-04-25 13:29:00</td>\n",
       "      <td>\"b\"\"rt @gopoversight: democrats are obsessed w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1121405495405948929</td>\n",
       "      <td>2019-04-25 13:27:00</td>\n",
       "      <td>\"rt @jim_jordan: peter strzok told us that he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1121388967444799488</td>\n",
       "      <td>2019-04-25 12:22:00</td>\n",
       "      <td>welcome to the race sleepy joe. i only hope yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1121385795481423873</td>\n",
       "      <td>2019-04-25 12:09:00</td>\n",
       "      <td>.....despite the fact that the mueller report ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  t_id              t_date  \\\n",
       "0  1121407497741447168 2019-04-25 13:35:00   \n",
       "1  1121407411875647488 2019-04-25 13:35:00   \n",
       "2  1121406614093860864 2019-04-25 13:32:00   \n",
       "3  1121406415707484160 2019-04-25 13:31:00   \n",
       "4  1121406299013513217 2019-04-25 13:31:00   \n",
       "5  1121406061519544322 2019-04-25 13:30:00   \n",
       "6  1121405753364025344 2019-04-25 13:29:00   \n",
       "7  1121405495405948929 2019-04-25 13:27:00   \n",
       "8  1121388967444799488 2019-04-25 12:22:00   \n",
       "9  1121385795481423873 2019-04-25 12:09:00   \n",
       "\n",
       "                                              t_text  \n",
       "0  rt @repandybiggsaz: i\\xe2\\x80\\x99m grateful fo...  \n",
       "1  rt @repmattgaetz: volume 1 of the mueller repo...  \n",
       "2  rt @repmarkmeadows: we knew they wouldn\\xe2\\x8...  \n",
       "3  \"rt @dcexaminer: \"\"the collusion delusion fell...  \n",
       "4  rt @repmarkmeadows: reminder: democrats demand...  \n",
       "5  rt @jim_jordan: it\\xe2\\x80\\x99s time to figure...  \n",
       "6  \"b\"\"rt @gopoversight: democrats are obsessed w...  \n",
       "7  \"rt @jim_jordan: peter strzok told us that he ...  \n",
       "8  welcome to the race sleepy joe. i only hope yo...  \n",
       "9  .....despite the fact that the mueller report ...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "475\n"
     ]
    }
   ],
   "source": [
    "# rt olanları cıkartalım.\n",
    "df3=df2 \\\n",
    "        .filter(~(\n",
    "                F.col(\"t_text\").contains(\"rt\")  \n",
    "))\n",
    "\n",
    "print(df2.count())\n",
    "print(df3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>t_date</th>\n",
       "      <th>t_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1121388967444799488</td>\n",
       "      <td>2019-04-25 12:22:00</td>\n",
       "      <td>welcome to the race sleepy joe. i only hope yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1121382698742841344</td>\n",
       "      <td>2019-04-25 11:57:00</td>\n",
       "      <td>....mueller was not fired and was respectfully...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1121187494312255489</td>\n",
       "      <td>2019-04-24 23:01:00</td>\n",
       "      <td>the great state of tennessee is so close to pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1121138875638792192</td>\n",
       "      <td>2019-04-24 19:48:00</td>\n",
       "      <td>.@senmikelee of the great state of utah has wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1121121705970286592</td>\n",
       "      <td>2019-04-24 18:40:00</td>\n",
       "      <td>as one united nation, we will work, we will pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1121120816983363584</td>\n",
       "      <td>2019-04-24 18:36:00</td>\n",
       "      <td>today, @flotus melania and i were honored to j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1121053578603397120</td>\n",
       "      <td>2019-04-24 14:09:00</td>\n",
       "      <td>i didn\\xe2\\x80\\x99t call bob costa of the wash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1121049166615142400</td>\n",
       "      <td>2019-04-24 13:52:00</td>\n",
       "      <td>....congress has no time to legislate, they on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1121048120312389634</td>\n",
       "      <td>2019-04-24 13:47:00</td>\n",
       "      <td>no collusion, no obstruction - there has never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1121044792945926144</td>\n",
       "      <td>2019-04-24 13:34:00</td>\n",
       "      <td>can anyone comprehend what a great job border ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  t_id               t_date  \\\n",
       "0  1121388967444799488  2019-04-25 12:22:00   \n",
       "1  1121382698742841344  2019-04-25 11:57:00   \n",
       "2  1121187494312255489  2019-04-24 23:01:00   \n",
       "3  1121138875638792192  2019-04-24 19:48:00   \n",
       "4  1121121705970286592  2019-04-24 18:40:00   \n",
       "5  1121120816983363584  2019-04-24 18:36:00   \n",
       "6  1121053578603397120  2019-04-24 14:09:00   \n",
       "7  1121049166615142400  2019-04-24 13:52:00   \n",
       "8  1121048120312389634  2019-04-24 13:47:00   \n",
       "9  1121044792945926144  2019-04-24 13:34:00   \n",
       "\n",
       "                                              t_text  \n",
       "0  welcome to the race sleepy joe. i only hope yo...  \n",
       "1  ....mueller was not fired and was respectfully...  \n",
       "2  the great state of tennessee is so close to pa...  \n",
       "3  .@senmikelee of the great state of utah has wr...  \n",
       "4  as one united nation, we will work, we will pr...  \n",
       "5  today, @flotus melania and i were honored to j...  \n",
       "6  i didn\\xe2\\x80\\x99t call bob costa of the wash...  \n",
       "7  ....congress has no time to legislate, they on...  \n",
       "8  no collusion, no obstruction - there has never...  \n",
       "9  can anyone comprehend what a great job border ...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bastan ve sondan boslukları silelim\n",
    "df4=df3 \\\n",
    "        .withColumn(\"t_id\",F.trim(F.col(\"t_id\"))) \\\n",
    "        .withColumn(\"t_date\",F.trim(F.col(\"t_date\"))) \\\n",
    "        .withColumn(\"t_text\",F.trim(F.col(\"t_text\"))) \n",
    "    \n",
    "df5.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|               t_id|             t_date|              t_text|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|1121388967444799488|2019-04-25 12:22:00|welcome to the ra...|\n",
      "|1121382698742841344|2019-04-25 11:57:00|....mueller was n...|\n",
      "|1121187494312255489|2019-04-24 23:01:00|the great state o...|\n",
      "|1121121705970286592|2019-04-24 18:40:00|as one united nat...|\n",
      "|1121120816983363584|2019-04-24 18:36:00|today, @flotus me...|\n",
      "|1121049166615142400|2019-04-24 13:52:00|....congress has ...|\n",
      "|1121048120312389634|2019-04-24 13:47:00|no collusion, no ...|\n",
      "|1121044792945926144|2019-04-24 13:34:00|can anyone compre...|\n",
      "|1121025624632647682|2019-04-24 12:18:00|.....are there no...|\n",
      "|1121021160827887616|2019-04-24 12:00:00|mexico\\xe2\\x80\\x9...|\n",
      "|1121006942502182913|2019-04-24 11:04:00|\\xe2\\x80\\x9cforme...|\n",
      "|1120851364035399680|2019-04-24 00:46:00|thanks rush! @fox...|\n",
      "|1120820536605585408|2019-04-23 22:43:00|you mean the stoc...|\n",
      "|1120793199650463747|2019-04-23 20:54:00|great meeting thi...|\n",
      "|1120751418414108680|2019-04-23 18:08:00|i will be in gree...|\n",
      "|1120651611410436098|2019-04-23 11:32:00|.....but should b...|\n",
      "|1120644639311134720|2019-04-23 11:04:00|\\xe2\\x80\\x9charle...|\n",
      "|1120642109676118016|2019-04-23 10:54:00|....dumb and sick...|\n",
      "|1120639589994106881|2019-04-23 10:44:00|sorry to say but ...|\n",
      "|1120635200327831552|2019-04-23 10:27:00|in the \\xe2\\x80\\x...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http removes\n",
    "# removing https\n",
    "expr = \"http.*\"\n",
    "df5 = df4.filter(~(F.col(\"t_text\").rlike(expr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|t_id|t_date|t_text|\n",
      "+----+------+------+\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check if there is any https\n",
    "df5.where(F.col(\"t_text\").like(\"%http%\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "|               t_id|             t_date|              t_text|          text_token|\n",
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "|1121138875638792192|2019-04-24 19:48:00|.@senmikelee of t...|[senmikelee, of, ...|\n",
      "|1121053578603397120|2019-04-24 14:09:00|i didn\\xe2\\x80\\x9...|[i, didn, xe2, x8...|\n",
      "|1120742807847800839|2019-04-23 17:34:00|great golf champi...|[great, golf, cha...|\n",
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "regexTokenizer = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 't_text', outputCol = 'text_token')\n",
    "df_token = regexTokenizer.transform(df5)\n",
    "df_token.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|               t_id|             t_date|              t_text|          text_token|     text_sw_removed|\n",
      "+-------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|1121138875638792192|2019-04-24 19:48:00|.@senmikelee of t...|[senmikelee, of, ...|[senmikelee, grea...|\n",
      "|1121053578603397120|2019-04-24 14:09:00|i didn\\xe2\\x80\\x9...|[i, didn, xe2, x8...|[didn, xe2, x80, ...|\n",
      "|1120742807847800839|2019-04-23 17:34:00|great golf champi...|[great, golf, cha...|[great, golf, cha...|\n",
      "|1120655455594799105|2019-04-23 11:47:00|keep america great!'|[keep, america, g...|[keep, america, g...|\n",
      "|1120655199130017792|2019-04-23 11:46:00|the wall is being...|[the, wall, is, b...|[wall, rapidly, b...|\n",
      "+-------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "swr = StopWordsRemover(inputCol = 'text_token', outputCol = 'text_sw_removed')\n",
    "tweet_swr = swr.transform(df_token)\n",
    "tweet_swr.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "#create an average word vector for each document\n",
    "word2vec = Word2Vec(minCount = 1, inputCol = 'text_sw_removed', outputCol = 'result')\n",
    "model = word2vec.fit(tweet_swr)\n",
    "result = model.transform(tweet_swr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               t_id|             t_date|              t_text|          text_token|     text_sw_removed|              result|\n",
      "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1121138875638792192|2019-04-24 19:48:00|.@senmikelee of t...|[senmikelee, of, ...|[senmikelee, grea...|[4.87581986051641...|\n",
      "|1121053578603397120|2019-04-24 14:09:00|i didn\\xe2\\x80\\x9...|[i, didn, xe2, x8...|[didn, xe2, x80, ...|[-2.5814099769507...|\n",
      "|1120742807847800839|2019-04-23 17:34:00|great golf champi...|[great, golf, cha...|[great, golf, cha...|[-0.0012052971214...|\n",
      "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+\n",
      "|              result|\n",
      "+--------------------+\n",
      "|[4.87581986051641...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(3)\n",
    "result.select('result').show(1, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t_id: string (nullable = true)\n",
      " |-- t_date: string (nullable = true)\n",
      " |-- t_text: string (nullable = true)\n",
      " |-- text_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text_sw_removed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- result: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()\n",
    "#result.write.saveAsTable(\"result\", format=\"parquet\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between two vectors \n",
    "def cossim(v1, v2): \n",
    "    return np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / np.sqrt(np.dot(v2, v2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_recommend(tweet_id, docvecs):\n",
    "    input_vec = docvecs.select('result').filter(docvecs['t_id'] == tweet_id).collect()[0][0]   \n",
    "    docvecs = docvecs.select('t_id','result').rdd.map(lambda x: (x[0], x[1])).collect()\n",
    "    \n",
    "    #compute similarity array\n",
    "    t = sc.parallelize((i[0], float(cossim(input_vec, i[1]))) for i in docvecs)\n",
    "    \n",
    "    # recommendation's cossim values\n",
    "    similarity = spark.createDataFrame(t).\\\n",
    "    withColumnRenamed('_1', 'tweet_id').\\\n",
    "    withColumnRenamed('_2', 'similarity').\\\n",
    "    orderBy(\"similarity\", ascending = False)\n",
    "  \n",
    "    return similarity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|tweet_id           |similarity        |\n",
      "+-------------------+------------------+\n",
      "|1121138875638792192|1.0               |\n",
      "|1102791903324631040|0.8312465787510966|\n",
      "|1107997913584726020|0.8298340329952292|\n",
      "|1102007681588563969|0.8219903746021223|\n",
      "|1115218802290122752|0.82022217914165  |\n",
      "|1103787204965478404|0.8051104511645759|\n",
      "|1114523496237883392|0.8010976712993788|\n",
      "|1105960676097441793|0.7678433874948783|\n",
      "|1111209625825640448|0.7441570601964962|\n",
      "|1105834434962571265|0.7205428236125595|\n",
      "|1114207309184602112|0.716563608100392 |\n",
      "|1106161655292010502|0.7149127738590835|\n",
      "|1108559080204001280|0.6795696823521122|\n",
      "|1102661631568461824|0.6271673827337592|\n",
      "|1106529365549084672|0.6235181291720807|\n",
      "|1100110413209858048|0.6116754976954513|\n",
      "|1104018553987100673|0.6042849130319551|\n",
      "|1114888062884954114|0.5761606924612956|\n",
      "|1112900816371900418|0.556188789385397 |\n",
      "|1121053578603397120|0.5434824179661336|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_recommend(\"1121138875638792192\", result).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(t_id='1121138875638792192', t_date='2019-04-24 19:48:00', t_text=\".@senmikelee of the great state of utah has written a wonderful new book entitled, \\\\xe2\\\\x80\\\\x9cour lost declaration.\\\\xe2\\\\x80\\\\x9d highly recommended!'\", text_token=['senmikelee', 'of', 'the', 'great', 'state', 'of', 'utah', 'has', 'written', 'a', 'wonderful', 'new', 'book', 'entitled', 'xe2', 'x80', 'x9cour', 'lost', 'declaration', 'xe2', 'x80', 'x9d', 'highly', 'recommended'], text_sw_removed=['senmikelee', 'great', 'state', 'utah', 'written', 'wonderful', 'new', 'book', 'entitled', 'xe2', 'x80', 'x9cour', 'lost', 'declaration', 'xe2', 'x80', 'x9d', 'highly', 'recommended'], result=DenseVector([0.0, -0.0004, 0.0012, -0.0031, 0.0009, 0.0007, -0.0015, -0.0005, -0.0006, 0.0007, 0.0004, -0.0008, 0.0012, 0.0003, -0.0003, -0.0007, 0.0015, -0.0005, 0.0004, 0.0024, -0.0013, -0.0025, -0.0013, -0.0033, -0.0011, -0.0005, 0.0008, -0.0008, 0.0014, 0.0, 0.0004, -0.0008, 0.0017, -0.0019, 0.0009, -0.0001, 0.0018, -0.0004, 0.0002, -0.0001, 0.0006, -0.0007, -0.0008, -0.0016, -0.0004, -0.0006, -0.0014, -0.0005, 0.0006, 0.0004, -0.001, -0.0001, -0.0004, -0.0004, -0.0001, 0.0011, 0.0001, -0.0001, -0.0002, -0.001, -0.0004, -0.0, 0.0018, 0.001, 0.001, -0.0017, 0.0009, -0.0009, 0.0003, 0.0009, 0.001, -0.0004, 0.0008, -0.0006, 0.0004, -0.0006, -0.0016, 0.0023, -0.0009, -0.0011, 0.0002, 0.0016, 0.0004, 0.0016, 0.0009, -0.0011, 0.0005, -0.0004, 0.0022, 0.0015, 0.0012, 0.0007, 0.0003, 0.0011, -0.0017, -0.0002, 0.0029, -0.0015, -0.0007, -0.0017]))]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.where(F.col(\"t_id\") == 1121138875638792192).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|    word|         similarity|\n",
      "+--------+-------------------+\n",
      "|  number|0.25912711024284363|\n",
      "| victory|0.25897228717803955|\n",
      "|standard| 0.2551022469997406|\n",
      "| illegal|  0.248518168926239|\n",
      "|  voting| 0.2445465326309204|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test similarity between words\n",
    "synonyms = model.findSynonyms(\"president\", 5)   # its okay for certain words (cuisines, foods), real bad for others\n",
    "synonyms.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|        word|         similarity|\n",
      "+------------+-------------------+\n",
      "|        done| 0.2754474878311157|\n",
      "|        lied|0.24875687062740326|\n",
      "|presidential|0.23190045356750488|\n",
      "|    employed|0.22457554936408997|\n",
      "|       world|0.22156548500061035|\n",
      "+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synonyms = model.findSynonyms(\"crazy\", 5)   # its okay for certain words (cuisines, foods), real bad for others\n",
    "synonyms.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test similarity between words\n",
    "def keyword_recommend(input_str, docvecs):\n",
    "    # run input_str through preprocessing pipeline\n",
    "    x = sc.parallelize([(1, input_str)]).toDF(['t_id', 't_text'])\n",
    "    from pyspark.ml.feature import RegexTokenizer\n",
    "    regexTokenizer = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 't_text', outputCol = 'text_token')\n",
    "    x_token = regexTokenizer.transform(x)\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "    swr = StopWordsRemover(inputCol = 'text_token', outputCol = 'text_sw_removed')\n",
    "    x_swr = swr.transform(x_token)\n",
    "    \n",
    "    # run word2vec model on input string\n",
    "    input_vec = model.transform(x_swr)\n",
    "    input_vec = input_vec.select('result').collect()[0][0]\n",
    "    \n",
    "    docvecs = docvecs.select('t_id','result').rdd.map(lambda x: (x[0], x[1])).collect()\n",
    "    \n",
    "    #compute similarity array\n",
    "    t = sc.parallelize((i[0], float(cossim(input_vec, i[1]))) for i in docvecs)\n",
    "    \n",
    "    # recommendation's cossim values\n",
    "    similarity = spark.createDataFrame(t).\\\n",
    "    withColumnRenamed('_1', 'tweet_id').\\\n",
    "    withColumnRenamed('_2', 'similarity').\\\n",
    "    orderBy(\"similarity\", ascending = False)\n",
    "  \n",
    "    return similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|tweet_id           |similarity          |\n",
      "+-------------------+--------------------+\n",
      "|1106554754715533313|0.5943848628534582  |\n",
      "|1102360081247682561|0.4244376060880388  |\n",
      "|1120093671356022785|0.3333256513802402  |\n",
      "|1104017664131907584|0.22012572493032603 |\n",
      "|1114156288555016193|0.15761668589896682 |\n",
      "|1117496743032250368|0.12131485974817567 |\n",
      "|1119962857347735552|0.11917203055649125 |\n",
      "|1114290701334802433|0.11301846708165722 |\n",
      "|1101289057060077569|0.10899428842914181 |\n",
      "|1101289772528603137|0.10329962419042553 |\n",
      "|1118317574301863936|0.09604921024046569 |\n",
      "|1104033876291305473|0.09472673678995967 |\n",
      "|1116157899724611584|0.08581757036733936 |\n",
      "|1117751619469361152|0.0802298430747358  |\n",
      "|1117843954437767168|0.07274550926957822 |\n",
      "|1106665436865851394|0.06353980612359773 |\n",
      "|1105871954001739782|0.060546781694689425|\n",
      "|1117486994018451458|0.05883977018714115 |\n",
      "|1108190837257764864|0.058780359227617984|\n",
      "|1117829011860787200|0.04988724600038479 |\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyword_recommend(\"president\", result).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
